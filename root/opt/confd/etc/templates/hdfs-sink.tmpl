name={{ getenv "SERVICE_NAME" "hdfs" }}-connector
connector.class=io.confluent.connect.hdfs.HdfsSinkConnector
tasks.max={{ getenv "TASKS" "1" }}
topics={{ getenv "TOPICS" "test_hdfs" }}

hdfs.url={{ getenv "HDFS_SERVICE" "hdfs://nameservice1" }}
hadoop.conf.dir=/opt/docker/conf/hadoop-conf
topics.dir={{ getenv "HDFS_PATH" "/data" }}
logs.dir={{ getenv "HDFS_LOGS_PATH" "/logs" }}
format.class=io.confluent.connect.hdfs.avro.AvroFormat
storage.class=io.confluent.connect.hdfs.storage.HdfsStorage

hive.integration={{ getenv "HIVE_INTEGRATION" "false" }}

hdfs.authentication.kerberos={{ getenv "KERBEROS_AUTH" "false" }}
connect.hdfs.principal={{ getenv "KERBEROS_PRINCIPAL" "principal@EXAMPLE.COM" }}
connect.hdfs.keytab={{ getenv "KERBEROS_KEYTAB" "/opt/docker/auth/principal.keytab" }}
hdfs.namenode.principal={{ getenv "KERBEROS_HDFS_PRINCIPAL" "hdfs/nameservice1@EXAMPLE.COM" }}
kerberos.ticket.renew.period.ms={{ getenv "KERBEROS_TICKET_RENEW" "3600000" }}

schema.compatibility={{ getenv "SCHEMA_COMPAT" "BACKWARD" }}
schema.cache.size={{ getenv "SCHEMA_CACHE" "1000" }}

flush.size={{ getenv "FLUSH_SIZE" "10000" }}
rotate.interval.ms={{ getenv "ROTATE_INTERVAL" "-1" }}
rotate.schedule.interval.ms={{ getenv "ROTATE_SCHEDULE_INTERVAL" "600000" }}
retry.backoff.ms={{ getenv "RETRY_BACKOFF" "10000" }}
shutdown.timeout.ms={{ getenv "SHUTDOWN_TIMEOUT" "3000" }}

partitioner.class={{ getenv "PARTITIONER" "io.confluent.connect.hdfs.partitioner.TimeBasedPartitioner" }}
partition.field.name={{ getenv "PARTITION_FIELD" "id" }}
partition.duration.ms={{ getenv "PARTITION_DURATION" "600000" }}
path.format={{ getenv "PARTITION_FORMAT" "YYYY/MM/dd/HH/" }}
locale={{ getenv "LOCALE" "en" }}
timezone={{ getenv "TIMEZONE" "UTC" }}
filename.offset.zero.pad.width={{ getenv "FILENAME_ZERO_PAD" "10" }}
timestamp.extractor=Record
